{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/dhe53dB6gX8EY/4yeYkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahulchunduruu/Machine-Learning/blob/main/Ragg_basic_appliaction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJtlKsQVQZJf",
        "outputId": "c5ce68d9-bce2-4c8e-d9c9-01d68be2da42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for RAG (Retrieval Augmented Generation) capabilities.\n",
        "# sentence-transformers: For embedding models.\n",
        "# langchain, langchain-community, langchain-core, langchain_google_genai, langchain_chroma: Core LangChain components for building LLM applications, including integrations for Google Generative AI and ChromaDB.\n",
        "# faiss-cpu: A library for efficient similarity search and clustering of dense vectors (though FAISS is later replaced by ChromaDB).\n",
        "# pypdf: For loading PDF documents.\n",
        "!pip install -q sentence-transformers langchain faiss-cpu langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional LangChain-related libraries, ensuring specific components are available.\n",
        "# langchain: The main LangChain library.\n",
        "# langchain-community: Community contributed LangChain integrations.\n",
        "# langchain-core: Core LangChain functionalities.\n",
        "# langchain_google_genai: Integration for Google's Generative AI models.\n",
        "# langchain_chroma: Integration for Chroma vector database.\n",
        "!pip install langchain langchain-community langchain-core langchain_google_genai langchain_chroma"
      ],
      "metadata": {
        "id": "KCzm4GrIiRpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from LangChain and other libraries.\n",
        "# PyPDFLoader: To load PDF documents.\n",
        "# os: For operating system functionalities (though not strictly used in this cell, it's good practice for API keys).\n",
        "# ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings: For interacting with Google's Generative AI models (chat and embeddings).\n",
        "# Chroma: The vector database for storing and retrieving document embeddings.\n",
        "# Document: A class to represent a document with content and metadata.\n",
        "# ChatPromptTemplate: For defining structured prompts for chat models.\n",
        "# RunnablePassthrough: A LangChain component to pass inputs through a chain.\n",
        "# StrOutputParser: To parse the string output from the LLM.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "iglkZzrfQjBr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PDF document using PyPDFLoader.\n",
        "# The loader takes the path to the PDF file.\n",
        "# loader.load() reads the content of the PDF and returns a list of Document objects, where each page is a separate document.\n",
        "loader = PyPDFLoader(\"/content/OceanofPDF.com_The_Genius_Myth_-_Helen_Lewis.pdf\")\n",
        "documents = loader.load()\n",
        "print(f\"loaded {len(documents)} pages from \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkRo2bFPhrU_",
        "outputId": "5008e468-bf91-40a5-f0e1-4b67225e1f21"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 287 pages from \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the type of the 'documents' variable to confirm it's a list.\n",
        "print(type(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhZwyWToh-2R",
        "outputId": "933b0d98-4aed-41e1-8852-202732324158"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the content and metadata of the first document (page) to inspect its structure.\n",
        "print(documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhhrXMF_iGAg",
        "outputId": "9dc35240-fbfa-4d8a-9481-2e501cb24fe3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='' metadata={'producer': 'calibre 7.4.0', 'creator': 'calibre 7.4.0', 'creationdate': '2025-06-19T23:32:49+00:00', 'author': 'Helen Lewis', 'moddate': '2025-06-19T23:32:49+00:00', 'title': 'The Genius Myth: A Curious History of a Dangerous Idea', 'source': '/content/OceanofPDF.com_The_Genius_Myth_-_Helen_Lewis.pdf', 'total_pages': 287, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the loaded documents into smaller, manageable chunks.\n",
        "# CharacterTextSplitter: A text splitter that splits text based on characters.\n",
        "# chunk_size = 500: Each chunk will have a maximum of 500 characters.\n",
        "# chunk_overlap = 50: There will be an overlap of 50 characters between consecutive chunks to maintain context.\n",
        "# The 'chunks' variable will store the list of these smaller document parts.\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "splitter = CharacterTextSplitter(chunk_size = 500,chunk_overlap = 50)\n",
        "chunks = splitter.split_documents(documents)\n",
        "print(f\"split into {len(chunks)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIWoLyWmiQyT",
        "outputId": "ca7aacf3-ac61-4e3b-ae68-98f16debbdc4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split into 285 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a HuggingFace embedding model.\n",
        "# HuggingFaceEmbeddings: A class to use models from HuggingFace for generating embeddings.\n",
        "# model_name = \"all-MiniLM-L6-v2\": Specifies the pre-trained model to use for embeddings, known for its efficiency and good performance.\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "fTqaervCicx-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector database from the document chunks.\n",
        "# FAISS: A library for efficient similarity search and clustering of dense vectors.\n",
        "# from_documents: A method to create a FAISS index directly from a list of documents and an embedding model.\n",
        "# The 'chunks' (smaller parts of the PDF) are converted into vector embeddings using 'embedding_model' and stored in 'vector_db'.\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_db = FAISS.from_documents(chunks, embedding=embedding_model)"
      ],
      "metadata": {
        "id": "eE1X9f2kiiev"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a similarity search in the FAISS vector database.\n",
        "# query: The search query (this line is commented out, assuming 'query' is defined elsewhere or will be from user input).\n",
        "# relevant_docs: Stores the top 'k' most similar documents found in the vector database.\n",
        "# k = 3: Specifies to retrieve the 3 most relevant document chunks.\n",
        "# The loop then prints the content of each retrieved chunk.\n",
        "#query = input(\"enter your question\")\n",
        "\n",
        "relevant_docs = vector_db.similarity_search(query,k = 4)\n",
        "\n",
        "for i,doc in enumerate(relevant_docs,1):\n",
        "  print(f\" chunk{i} ----\\n{doc.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9lufgOCimRZ",
        "outputId": "1e882f30-5fd0-486b-c26a-bd8d3e17d7ba"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " chunk1 ----\n",
            "ALSO  BY  HELEN  LEWIS\n",
            "Difficult Women\n",
            "The Spark\n",
            "OceanofPDF.com\n",
            "\n",
            " chunk2 ----\n",
            "Finally, to the one person who makes everything possible: Jonathan. I\n",
            "promise I won’t write another book. Unless I have a really, really good idea.\n",
            "No! I promise. Probably.\n",
            "OceanofPDF.com\n",
            "\n",
            " chunk3 ----\n",
            "4. James Baldwin, radio interview with Studs Terkel, 1961. Available at\n",
            "https://studsterkel.wfmt.com/programs/james-baldwin-discusses-his-\n",
            "book-nobody-knows-my-name-more-notes-native-son\n",
            "BACK TO NOTE REFERENCE 4\n",
            "OceanofPDF.com\n",
            "\n",
            " chunk4 ----\n",
            "outstanding book Control deals with the history of eugenics, was a cheering\n",
            "presence; who else could I talk to about Francis Galton? Stuart Ritchie\n",
            "kindly helped me understand some of the key bits of intelligence research.\n",
            "Chris Morris explained Prince’s musical magic and told me about Galton’s\n",
            "cake trick. Cordelia Fine sent me useful links on sex differences and\n",
            "intelligence. Chris Kavanagh was a sounding board on the subject of\n",
            "charismatic charlatans. Craig Brown not only talked to me about the\n",
            "Beatles, but offered me an excellent lunch at his house in Aldeburgh\n",
            "(though I was sad not to play him at ping-pong; apparently, he’s a demon).\n",
            "Janice Turner let me stay in her house by the seaside at a particularly low\n",
            "point; Laura McInerney’s energising hatred of productivity gurus was a\n",
            "regular delight. Ian Leslie’s work on influence and creativity, and his love\n",
            "of the Beatles, unlocked so many doors for me; as did Henry Oliver’s\n",
            "writing on late bloomers. Richard Morris and Gwyn Davies, the producers\n",
            "of my BBC series Great Wives, pushed me to look at Sofia Tolstaya and a\n",
            "host of other women who lived with the dark side of genius; I’ve borrowed\n",
            "from those scripts for the chapter. The intellectual salon no longer exists, so\n",
            "instead Gia, Tracy, Caroline, Becca, Hadley, Janice and Sarah gave me\n",
            "constant support and inspiration on its replacement, WhatsApp.\n",
            "I would also like to thank all my interviewees, particularly those who\n",
            "shared personal memories of Chris Goode, speaking honestly and\n",
            "empathetically about a difficult period in their lives. The fundamental\n",
            "decency shown by Simon Stephens will always stay with me. Jack Barth\n",
            "was generous with his time talking about an incident I’m sure he would\n",
            "rather forget. So many authors have already mapped out the terrain I have\n",
            "covered, and I owe them all a huge debt: this entire book stands on the\n",
            "shoulders of giants. And a couple of frauds.\n",
            "Robert Icke pushed me to pick this topic, out of all the topics in the\n",
            "world, and Alex Garland argued with me about the status of Jane Austen.\n",
            "Watching both of them work has opened my eyes to the collision of\n",
            "creativity, commerce and sheer luck involved in making great art. The book\n",
            "is dedicated to them.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the FAISS vector database object to confirm its creation and type.\n",
        "print(vector_db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BS5Kodap_Cy",
        "outputId": "26dc6e06-447e-4a51-d611-a653fbcd2c90"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain_community.vectorstores.faiss.FAISS object at 0x7964b537fc20>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDrp-ilKkSi-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'os' module, which provides a way of using operating system dependent functionality.\n",
        "# This is often used for managing environment variables or file paths.\n",
        "import os"
      ],
      "metadata": {
        "id": "IzmOik3VnXMU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Set the Google API key as an environment variable.\n",
        "# This is crucial for authenticating with Google services like Generative AI.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD5mrY_jD_LLWf4P3uLjLgSmFqTWHK3jU4\"\n",
        "\n",
        "# --- 1. SETUP MODELS ---\n",
        "# We need two models:\n",
        "# A. Embedding Model: Converts text into vector numbers.\n",
        "# GoogleGenerativeAIEmbeddings: Uses Google's model to convert text into numerical vector representations.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "\n",
        "# B. LLM: The chat model that will answer the question.\n",
        "# ChatGoogleGenerativeAI: Initializes Google's Gemini 2.0 Flash chat model.\n",
        "# temperature=0: Sets the model's creativity level to minimum, making responses more deterministic.\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(\"--- Initializing Models and Embeddings ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVcjZOCtkOCR",
        "outputId": "d2867f60-d2a1-4a96-b15d-39d0ba09b0e0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Models and Embeddings ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. BUILD VECTOR STORE (The 'Link') ---\n",
        "# This step:\n",
        "# 1. Takes the document chunks.\n",
        "# 2. Uses the 'embeddings' model to turn them into vectors.\n",
        "# 3. Stores them in ChromaDB (in memory for this example).\n",
        "# Chroma.from_documents: Creates a Chroma vector store from the 'chunks' (processed document parts).\n",
        "# documents=chunks: The list of document chunks to be embedded and stored.\n",
        "# embedding=embeddings: The embedding model to use for converting text to vectors.\n",
        "# collection_name=\"gemini_knowledge_base\": A name for the collection of documents within ChromaDB.\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks, # Use the 'chunks' created earlier\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"gemini_knowledge_base\"\n",
        ")"
      ],
      "metadata": {
        "id": "3JT_2xu2n3l0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a \"Retriever\" interface from the Chroma vector store.\n",
        "# vectorstore.as_retriever: Configures the vector store to act as a retriever.\n",
        "# search_kwargs={\"k\": 2}: Specifies that the retriever should return the top 2 most relevant document chunks.\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 results\n",
        "\n",
        "# --- 4. DEFINE THE RAG CHAIN ---\n",
        "# This is the \"brain\" that connects the DB to the LLM.\n",
        "\n",
        "# A simple prompt template for the LLM.\n",
        "# {context}: Placeholder for the retrieved relevant document chunks.\n",
        "# {question}: Placeholder for the user's query.\n",
        "template = \"\"\"\n",
        "Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# A helper function to format the retrieved documents into a single string.\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# The RAG Chain definition:\n",
        "# This chain orchestrates the RAG process:\n",
        "# 1. \"context\": retriever | format_docs: The retriever fetches relevant documents, which are then formatted.\n",
        "# 2. \"question\": RunnablePassthrough(): The original question is passed through as is.\n",
        "# 3. | prompt: The formatted context and question are passed to the prompt template.\n",
        "# 4. | llm: The prompt is sent to the Large Language Model for generation.\n",
        "# 5. | StrOutputParser(): The LLM's output is parsed as a simple string.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- 5. RUN THE PIPELINE ---\n",
        "# Define the user's query.\n",
        "query = \"summary about this book\"\n",
        "print(f\"\\nUser Question: {query}\")\n",
        "print(\"Thinking...\")\n",
        "\n",
        "# Invoke the RAG chain with the query to get a response.\n",
        "response = rag_chain.invoke(query)\n",
        "\n",
        "print(f\"\\nGemini Answer:\\n{response}\")\n",
        "\n",
        "# Cleanup (optional)\n",
        "# vectorstore.delete_collection() # Commented out to keep the vector store for further queries in the session."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuOwzjahoSxZ",
        "outputId": "3bc6d4e7-c29d-4743-d5a5-7bee800cfc88"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Question: summary about this book\n",
            "Thinking...\n",
            "\n",
            "Gemini Answer:\n",
            "The provided text is an advertisement for a service that helps you find your next book. It doesn't describe a specific book, but rather offers personalized book recommendations and news about authors.\n"
          ]
        }
      ]
    }
  ]
}